services:
  # Redis cache for deduplication and caching recommendation history
  redis:
    image: redis:7-alpine
    platform: linux/amd64
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # PostgreSQL database with pgvector extension storing catalog, student, and checkout data for the entire system.
  postgres:
    image: ankane/pgvector:latest
    platform: linux/amd64
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: books
      POSTGRES_USER: books
      POSTGRES_PASSWORD: books
    volumes:
      - ./sql:/docker-entrypoint-initdb.d
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U books"]
      interval: 5s
      timeout: 5s
      retries: 5


  # Coordinates and manages Kafka broker metadata for event-driven services.
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    platform: linux/amd64
    hostname: zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: "ruok,stat"
    healthcheck:
      test: echo "ruok" | nc -w 2 localhost 2181 | grep imok
      interval: 5s
      timeout: 5s
      retries: 5

# Primary Kafka broker that transports events, logs, and metrics between microservices.
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    platform: linux/amd64
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
    depends_on: [zookeeper]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
    healthcheck:
      test: ["CMD", "bash", "-ec", "/usr/bin/kafka-topics --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 6

  # "Loads seed CSV data, populates PostgreSQL tables, and builds/updates the FAISS vector store for books."
  ingestion_service:
    build:
      context: .
      dockerfile: src/ingestion_service/Dockerfile
    platform: linux/amd64
    container_name: ingestion_service
    env_file:
      - .env
    volumes:
      - ./data:/app/data
    environment:
      - OPENAI_API_KEY
      - OPENAI_MODEL=${OPENAI_MODEL}
      - VECTOR_STORE_TYPE=${VECTOR_STORE_TYPE}
      - DB_URL=${DB_URL}
      - KAFKA_BROKERS=${KAFKA_BROKERS}
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started

  # FastAPI service exposing book-recommendation endpoints and registering MCP tools for LLM agents.
  recommendation_api:
    build:
      context: .
      dockerfile: src/recommendation_api/Dockerfile
    platform: linux/amd64
    container_name: recommendation_api
    env_file:
      - .env
    volumes:
      - ./data:/app/data
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY
      - OPENAI_MODEL=${OPENAI_MODEL}
      - VECTOR_STORE_TYPE=${VECTOR_STORE_TYPE}
      - DB_URL=${DB_URL}
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      - PYTHONPATH=/app:/app/common
    depends_on:
      ingestion_service:
        condition: service_started
      redis:
        condition: service_healthy

  # Production-ready LLM microservice with Redis caching, Kafka logging, and LangChain integration
  llm_microservice:
    build:
      context: .
      dockerfile: src/llm_microservice/Dockerfile
    platform: linux/amd64
    container_name: llm_microservice
    env_file:
      - .env
    ports:
      - "8001:8000"
    environment:
      - OPENAI_API_KEY
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      - REDIS_URL=redis://redis:6379
      - KAFKA_BROKERS=kafka:9092
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - CACHE_TTL=${CACHE_TTL:-3600}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-30}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy


# Streamlit dashboard that teachers and students use to search the catalog and receive recommendations.
  streamlit_ui:
    build:
      context: .
      dockerfile: src/streamlit_ui/Dockerfile
    platform: linux/amd64
    container_name: streamlit_ui
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs:ro
    ports:
      - "8501:8501"
    depends_on: [recommendation_api]
    
  # React + Vite single-page application served via Nginx
  frontend:
    build:
      context: ./src/react_ui
    platform: linux/amd64
    container_name: frontend
    depends_on:
      - recommendation_api
    ports:
      - "8080:80"
    # No env vars needed; Nginx proxies /api/* to recommendation_api
    # If you ever rename the backend service, update nginx.conf accordingly.
    

  # ---- READER MODE SERVICES ----
  # Handles user book uploads (JSON/CSV), validates data, enriches metadata, and publishes events
  user_ingest_service:
    build:
      context: .
      dockerfile: src/user_ingest_service/Dockerfile
    platform: linux/amd64
    container_name: user_ingest_service
    env_file:
      - .env
    ports:
      - "8004:8004"
    environment:
      - OPENAI_API_KEY
      - OPENAI_MODEL=${OPENAI_MODEL}
      - DB_URL=${DB_URL}
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      - REDIS_URL=${REDIS_URL}
      - ENABLE_READER_MODE=${ENABLE_READER_MODE}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started
      redis:
        condition: service_healthy
    deploy:
      replicas: ${ENABLE_READER_MODE:-1}

  # Processes user feedback events, updates database, and maintains recommendation scoring
  feedback_worker:
    build:
      context: .
      dockerfile: src/feedback_worker/Dockerfile
    platform: linux/amd64
    container_name: feedback_worker
    env_file:
      - .env
    environment:
      - DB_URL=${DB_URL}
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      - REDIS_URL=${REDIS_URL}
      - ENABLE_READER_MODE=${ENABLE_READER_MODE}
    healthcheck:
      test: ["CMD", "pgrep", "-f", "python"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started
      redis:
        condition: service_healthy
    deploy:
      replicas: ${ENABLE_READER_MODE:-1}

  
  # Kafka consumer that aggregates metrics events for monitoring and performance dashboards.
  metrics_consumer:
    build:
      context: .
      dockerfile: src/metrics_consumer/Dockerfile
    platform: linux/amd64
    container_name: metrics_consumer
    env_file:
      - .env
    environment:
      - KAFKA_BROKERS=${KAFKA_BROKERS}
    depends_on:
      kafka:
        condition: service_healthy
    

  # Consumes structured log events from Kafka and writes log files to the shared volume for debugging.
  log_consumer:
    build:
      context: .
      dockerfile: src/log_consumer/Dockerfile
    platform: linux/amd64
    container_name: log_consumer
    env_file:
      - .env
    environment:
      - KAFKA_BROKERS=${KAFKA_BROKERS}
    volumes:
      - ./logs:/app/logs
    depends_on:
      kafka:
        condition: service_healthy


    # Periodically recomputes similarity graphs and caches derived embeddings for recommendation algorithms.
  graph_refresher:
    build:
      context: .
      dockerfile: src/graph_refresher/Dockerfile
    platform: linux/amd64
    container_name: graph_refresher
    env_file:
      - .env
    volumes:
      - ./data:/app/data
    environment:
      - OPENAI_API_KEY
      - SIMILARITY_THRESHOLD=${SIMILARITY_THRESHOLD}
      - HALF_LIFE_DAYS=${HALF_LIFE_DAYS}
    depends_on:
      postgres:
        condition: service_started
      kafka:
        condition: service_healthy

  # Enriches catalog entries with external metadata (e.g., Google Books, Open Library) and readability scores.
  # Enhanced with priority-based processing, retry logic, and comprehensive status tracking.
  book_enrichment_worker:
    build:
      context: .
      dockerfile: src/book_enrichment_worker/Dockerfile
    platform: linux/amd64
    container_name: book_enrichment_worker
    env_file:
      - .env
    volumes:
      - ./data:/app/data
    environment:
      - OPENAI_API_KEY
      # Enrichment system configuration
      - ENRICHMENT_BATCH_SIZE=50
      - ENRICHMENT_BATCH_INTERVAL=30
      - ENRICHMENT_MAX_RETRIES_CRITICAL=5
      - ENRICHMENT_MAX_RETRIES_HIGH=3
      - ENRICHMENT_MAX_RETRIES_LOW=2
      - ENRICHMENT_RATE_LIMIT_CRITICAL=0.1
      - ENRICHMENT_RATE_LIMIT_HIGH=0.2
      - ENRICHMENT_RATE_LIMIT_LOW=0.5
      - ENRICHMENT_API_TIMEOUT=10
      - OPENLIBRARY_BASE_URL=https://openlibrary.org
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy


  # Generates and updates vector embeddings for books when catalog change events are received.
  book_vector_worker:
    build:
      context: .
      dockerfile: src/incremental_workers/book_vector/Dockerfile
    platform: linux/amd64
    container_name: book_vector_worker
    env_file:
      - .env
    volumes:
      - ./data:/app/data
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started

  # Updates cached student reading-level histograms upon checkout events and publishes profile change notifications.
  student_profile_worker:
    build:
      context: .
      dockerfile: src/incremental_workers/student_profile/Dockerfile
    platform: linux/amd64
    container_name: student_profile_worker
    env_file:
      - .env
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started

  # Creates vector embeddings representing individual students' reading preferences for similarity computations.
  student_embedding_worker:
    build:
      context: .
      dockerfile: src/incremental_workers/student_embedding/Dockerfile
    platform: linux/amd64
    container_name: student_embedding_worker
    env_file:
      - .env
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started

  # Computes and stores similarity scores between students based on their embedding vectors.
  similarity_worker:
    build:
      context: .
      dockerfile: src/incremental_workers/similarity/Dockerfile
    platform: linux/amd64
    container_name: similarity_worker
    env_file:
      - .env
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_started

  # ---- optional stubs ---------------------------------
  # Placeholder stub for future text-to-speech generation capability (inactive when replicas=0).
  tts_worker:
    build:
      context: .
      dockerfile: src/stubs/tts_worker/Dockerfile
    platform: linux/amd64
    container_name: tts_worker
    env_file:
      - .env
    environment:
      - ENABLE_TTS
    command: ["sleep","3600"]
    deploy:
      replicas: ${ENABLE_TTS}

  # Placeholder stub for future book-cover or illustration image generation service (inactive when replicas=0).
  image_worker:
    build:
      context: .
      dockerfile: src/stubs/image_worker/Dockerfile
    platform: linux/amd64
    container_name: image_worker
    env_file:
      - .env
    environment:
      - ENABLE_IMAGE
    command: ["sleep","3600"]
    deploy:
      replicas: ${ENABLE_IMAGE}

# ---------------- Monitoring stack -------------------------------------
  prometheus:
    image: prom/prometheus:latest
    platform: linux/amd64
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    depends_on:
      kafka:
        condition: service_healthy

  # Grafana for visualization and dashboards
  grafana:
    image: grafana/grafana:latest
    platform: linux/amd64
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus

volumes:
  # Database persistence (essential for demo session)
  pgdata: 
  
  # Monitoring stack (useful for showing metrics during demo)
  prometheus_data:
  grafana_data: